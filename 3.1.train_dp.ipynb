{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Diffusion Policy on your Dataset\n",
    "Train the Diffusion Policy model on your custom dataset. In this example, we set horizon size as 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.common.datasets.factory import resolve_delta_timestamps\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Number of offline training steps (we'll only do offline training for this example.)\n",
    "# Adjust as you prefer. 5000 steps are needed to get something worth evaluating.\n",
    "training_steps = 3000\n",
    "log_freq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Configuration and Initialize\n",
    "\n",
    "horizon = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Device 'None' is not available. Switching to 'cuda'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The horizon should be an integer multiple of the downsampling factor (which is determined by `len(down_dims)`). Got self.horizon=10 and self.down_dims=(512, 1024, 2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m input_features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.wrist_image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Policies are initialized with a configuration class, in this case . For this example,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# we'll just use the defaults and so no arguments other than input/output features need to be passed.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_action_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# This allows us to construct the data with action chunking\u001b[39;00m\n\u001b[1;32m     16\u001b[0m delta_timestamps \u001b[38;5;241m=\u001b[39m resolve_delta_timestamps(cfg, dataset_metadata)\n",
      "File \u001b[0;32m<string>:40\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, n_obs_steps, normalization_mapping, input_features, output_features, device, use_amp, horizon, n_action_steps, drop_n_last_frames, vision_backbone, crop_shape, crop_is_random, pretrained_backbone_weights, use_group_norm, spatial_softmax_num_keypoints, use_separate_rgb_encoder_per_camera, down_dims, kernel_size, n_groups, diffusion_step_embed_dim, use_film_scale_modulation, noise_scheduler_type, num_train_timesteps, beta_schedule, beta_start, beta_end, prediction_type, clip_sample, clip_sample_range, num_inference_steps, do_mask_loss_for_padding, optimizer_lr, optimizer_betas, optimizer_eps, optimizer_weight_decay, scheduler_name, scheduler_warmup_steps)\u001b[0m\n",
      "File \u001b[0;32m~/projects/lerobot/lerobot/common/policies/diffusion/configuration_diffusion.py:187\u001b[0m, in \u001b[0;36mDiffusionConfig.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m downsampling_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_dims)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhorizon \u001b[38;5;241m%\u001b[39m downsampling_factor \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe horizon should be an integer multiple of the downsampling factor (which is determined \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby `len(down_dims)`). Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhorizon\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_dims\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The horizon should be an integer multiple of the downsampling factor (which is determined by `len(down_dims)`). Got self.horizon=10 and self.down_dims=(512, 1024, 2048)"
     ]
    }
   ],
   "source": [
    "# When starting from scratch (i.e. not from a pretrained policy), we need to specify 2 things before\n",
    "# creating the policy:\n",
    "#   - input/output shapes: to properly size the policy\n",
    "#   - dataset stats: for normalization and denormalization of input/outputs\n",
    "dataset_metadata = LeRobotDatasetMetadata(\"omy_pnp\", root='./demo_data')\n",
    "features = dataset_to_policy_features(dataset_metadata.features)\n",
    "output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "input_features = {key: ft for key, ft in features.items() if key not in output_features}\n",
    "input_features.pop(\"observation.wrist_image\")\n",
    "\n",
    "# Policies are initialized with a configuration class, in this case . For this example,\n",
    "# we'll just use the defaults and so no arguments other than input/output features need to be passed.\n",
    "cfg = DiffusionConfig(input_features=input_features, output_features=output_features, horizon=8, n_action_steps=8)\n",
    "\n",
    "# This allows us to construct the data with action chunking\n",
    "delta_timestamps = resolve_delta_timestamps(cfg, dataset_metadata)\n",
    "\n",
    "# We can now instantiate our policy with this config and the dataset stats.\n",
    "policy = DiffusionPolicy(cfg, dataset_stats=dataset_metadata.stats)\n",
    "policy.train()\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., std=0.01):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # Adds noise: tensor remains a tensor.\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        return tensor + noise\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "# Create a transformation pipeline that converts a PIL image to a tensor, then adds noise.\n",
    "transform = transforms.Compose([\n",
    "    AddGaussianNoise(mean=0., std=0.02),\n",
    "    transforms.Lambda(lambda x: x.clamp(0, 1))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'delta_timestamps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We can then instantiate the dataset with these delta_timestamps configuration.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m LeRobotDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momy_pnp\u001b[39m\u001b[38;5;124m\"\u001b[39m, delta_timestamps\u001b[38;5;241m=\u001b[39m\u001b[43mdelta_timestamps\u001b[49m, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./demo_data\u001b[39m\u001b[38;5;124m'\u001b[39m, image_transforms\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Then we create our optimizer and dataloader for offline training.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(policy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'delta_timestamps' is not defined"
     ]
    }
   ],
   "source": [
    "# We can then instantiate the dataset with these delta_timestamps configuration.\n",
    "dataset = LeRobotDataset(\"omy_pnp\", delta_timestamps=delta_timestamps, root='./demo_data', image_transforms=transform)\n",
    "\n",
    "# Then we create our optimizer and dataloader for offline training.\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The trained checkpoint will be saved in './ckpt/diffusion_y' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training loop.\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    for batch in dataloader:\n",
    "        inp_batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        loss, _ = policy.forward(inp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % log_freq == 0:\n",
    "            print(f\"step: {step} loss: {loss.item():.3f}\")\n",
    "        step += 1\n",
    "        if step >= training_steps:\n",
    "            done = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the policy to disk.\n",
    "policy.save_pretrained('./ckpt/diffusion_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference\n",
    "\n",
    "To evaluate the policy on the dataset, you can calculate the error between ground-truth actions from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EpisodeSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset: LeRobotDataset, episode_index: int):\n",
    "        from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "        to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "        self.frame_ids = range(from_idx, to_idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.frame_ids)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frame_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "actions = []\n",
    "gt_actions = []\n",
    "images = []\n",
    "episode_index = 0\n",
    "episode_sampler = EpisodeSampler(dataset, episode_index)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    sampler=episode_sampler,\n",
    ")\n",
    "policy.reset()\n",
    "for batch in test_dataloader:\n",
    "    inp_batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "    action = policy.select_action(inp_batch)\n",
    "    actions.append(action)\n",
    "    gt_actions.append(inp_batch[\"action\"][:,0,:])\n",
    "    images.append(inp_batch[\"observation.image\"])\n",
    "actions = torch.cat(actions, dim=0)\n",
    "gt_actions = torch.cat(gt_actions, dim=0)\n",
    "print(f\"Mean action error: {torch.mean(torch.abs(actions - gt_actions)).item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
