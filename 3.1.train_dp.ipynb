{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Diffusion Policy on your Dataset\n",
    "Train the Diffusion Policy model on your custom dataset. In this example, we set horizon size as 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.common.datasets.factory import resolve_delta_timestamps\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Number of offline training steps (we'll only do offline training for this example.)\n",
    "# Adjust as you prefer. 5000 steps are needed to get something worth evaluating.\n",
    "training_steps = 3000\n",
    "log_freq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Configuration and Initialize\n",
    "\n",
    "horizon = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Device 'None' is not available. Switching to 'cuda'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiffusionPolicy(\n",
       "  (normalize_inputs): Normalize(\n",
       "    (buffer_observation_image): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.cuda.FloatTensor of size 3x1x1 (cuda:0)]\n",
       "        (std): Parameter containing: [torch.cuda.FloatTensor of size 3x1x1 (cuda:0)]\n",
       "    )\n",
       "    (buffer_observation_state): ParameterDict(\n",
       "        (max): Parameter containing: [torch.cuda.FloatTensor of size 6 (cuda:0)]\n",
       "        (min): Parameter containing: [torch.cuda.FloatTensor of size 6 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (normalize_targets): Normalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (max): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "        (min): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (unnormalize_outputs): Unnormalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (max): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "        (min): Parameter containing: [torch.cuda.FloatTensor of size 7 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       "  (diffusion): DiffusionModel(\n",
       "    (rgb_encoder): DiffusionRgbEncoder(\n",
       "      (center_crop): CenterCrop(size=(84, 84))\n",
       "      (maybe_random_crop): RandomCrop(size=(84, 84), padding=None)\n",
       "      (backbone): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (4): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pool): SpatialSoftmax(\n",
       "        (nets): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (unet): DiffusionConditionalUnet1d(\n",
       "      (diffusion_step_encoder): Sequential(\n",
       "        (0): DiffusionSinusoidalPosEmb()\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): Mish()\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (down_modules): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(7, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=1024, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(7, 512, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=1024, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=2048, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=2048, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=4096, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=4096, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (mid_modules): ModuleList(\n",
       "        (0-1): 2 x DiffusionConditionalResidualBlock1d(\n",
       "          (conv1): DiffusionConv1dBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "              (2): Mish()\n",
       "            )\n",
       "          )\n",
       "          (cond_encoder): Sequential(\n",
       "            (0): Mish()\n",
       "            (1): Linear(in_features=268, out_features=4096, bias=True)\n",
       "          )\n",
       "          (conv2): DiffusionConv1dBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "              (2): Mish()\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Identity()\n",
       "        )\n",
       "      )\n",
       "      (up_modules): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(4096, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=2048, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(4096, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=2048, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): ConvTranspose1d(1024, 1024, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2048, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=1024, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=268, out_features=1024, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): DiffusionConv1dBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (2): Mish()\n",
       "          )\n",
       "        )\n",
       "        (1): Conv1d(512, 7, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When starting from scratch (i.e. not from a pretrained policy), we need to specify 2 things before\n",
    "# creating the policy:\n",
    "#   - input/output shapes: to properly size the policy\n",
    "#   - dataset stats: for normalization and denormalization of input/outputs\n",
    "dataset_metadata = LeRobotDatasetMetadata(\"omy_pnp\", root='./demo_data')\n",
    "features = dataset_to_policy_features(dataset_metadata.features)\n",
    "output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "input_features = {key: ft for key, ft in features.items() if key not in output_features}\n",
    "input_features.pop(\"observation.wrist_image\")\n",
    "\n",
    "# Policies are initialized with a configuration class, in this case . For this example,\n",
    "# we'll just use the defaults and so no arguments other than input/output features need to be passed.\n",
    "cfg = DiffusionConfig(input_features=input_features, output_features=output_features, horizon=8, n_action_steps=8)\n",
    "\n",
    "# This allows us to construct the data with action chunking\n",
    "delta_timestamps = resolve_delta_timestamps(cfg, dataset_metadata)\n",
    "\n",
    "# We can now instantiate our policy with this config and the dataset stats.\n",
    "policy = DiffusionPolicy(cfg, dataset_stats=dataset_metadata.stats)\n",
    "policy.train()\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., std=0.01):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # Adds noise: tensor remains a tensor.\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        return tensor + noise\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "# Create a transformation pipeline that converts a PIL image to a tensor, then adds noise.\n",
    "transform = transforms.Compose([\n",
    "    AddGaussianNoise(mean=0., std=0.02),\n",
    "    transforms.Lambda(lambda x: x.clamp(0, 1))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then instantiate the dataset with these delta_timestamps configuration.\n",
    "dataset = LeRobotDataset(\"omy_pnp\", delta_timestamps=delta_timestamps, root='./demo_data', image_transforms=transform)\n",
    "\n",
    "# Then we create our optimizer and dataloader for offline training.\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The trained checkpoint will be saved in './ckpt/diffusion_y' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 1.172\n",
      "step: 100 loss: 0.126\n",
      "step: 200 loss: 0.078\n",
      "step: 300 loss: 0.069\n",
      "step: 400 loss: 0.052\n",
      "step: 500 loss: 0.043\n",
      "step: 600 loss: 0.034\n",
      "step: 700 loss: 0.032\n",
      "step: 800 loss: 0.049\n",
      "step: 900 loss: 0.037\n",
      "step: 1000 loss: 0.024\n",
      "step: 1100 loss: 0.033\n",
      "step: 1200 loss: 0.031\n",
      "step: 1300 loss: 0.036\n",
      "step: 1400 loss: 0.023\n",
      "step: 1500 loss: 0.027\n",
      "step: 1600 loss: 0.021\n",
      "step: 1700 loss: 0.022\n",
      "step: 1800 loss: 0.035\n",
      "step: 1900 loss: 0.028\n",
      "step: 2000 loss: 0.026\n",
      "step: 2100 loss: 0.024\n",
      "step: 2200 loss: 0.022\n",
      "step: 2300 loss: 0.029\n",
      "step: 2400 loss: 0.020\n",
      "step: 2500 loss: 0.029\n",
      "step: 2600 loss: 0.018\n",
      "step: 2700 loss: 0.018\n",
      "step: 2800 loss: 0.035\n",
      "step: 2900 loss: 0.016\n"
     ]
    }
   ],
   "source": [
    "# Run training loop.\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    for batch in dataloader:\n",
    "        inp_batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        loss, _ = policy.forward(inp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % log_freq == 0:\n",
    "            print(f\"step: {step} loss: {loss.item():.3f}\")\n",
    "        step += 1\n",
    "        if step >= training_steps:\n",
    "            done = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the policy to disk.\n",
    "policy.save_pretrained('./ckpt/diffusion_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference\n",
    "\n",
    "To evaluate the policy on the dataset, you can calculate the error between ground-truth actions from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EpisodeSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset: LeRobotDataset, episode_index: int):\n",
    "        from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "        to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "        self.frame_ids = range(from_idx, to_idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.frame_ids)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frame_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([1, 8, 7])\n",
      "Ground truth shape: torch.Size([1, 8, 7])\n",
      "Final shapes - Action: torch.Size([1, 8, 7]), GT: torch.Size([1, 7])\n",
      "Mean action error: 0.006\n"
     ]
    }
   ],
   "source": [
    "policy.eval()\n",
    "actions = []\n",
    "gt_actions = []\n",
    "images = []\n",
    "episode_index = 0\n",
    "episode_sampler = EpisodeSampler(dataset, episode_index)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    sampler=episode_sampler,\n",
    ")\n",
    "policy.reset()\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    inp_batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "    action = policy.select_action(inp_batch)\n",
    "    \n",
    "    # 모델 출력의 크기 확인\n",
    "    print(f\"Model output shape: {action.shape}\")\n",
    "    print(f\"Ground truth shape: {inp_batch['action'].shape}\")\n",
    "    \n",
    "    # 첫 번째 배치만 처리하고 중단 (디버깅용)\n",
    "    actions.append(action)\n",
    "    # 모델 출력과 같은 크기의 ground truth만 사용\n",
    "    gt_action = inp_batch[\"action\"][:, 0, :action.shape[1]]\n",
    "    gt_actions.append(gt_action)\n",
    "    images.append(inp_batch[\"observation.image\"])\n",
    "    break\n",
    "\n",
    "# 배치가 하나만 있으므로 간단히 인덱싱\n",
    "action = actions[0]  \n",
    "gt_action = gt_actions[0]\n",
    "\n",
    "print(f\"Final shapes - Action: {action.shape}, GT: {gt_action.shape}\")\n",
    "print(f\"Mean action error: {torch.mean(torch.abs(action - gt_action)).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action shape: torch.Size([8, 7])\n",
      "GT action shape: torch.Size([1, 7])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGT action shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_action\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# 액션 차원 수\u001b[39;00m\n\u001b[1;32m     13\u001b[0m time_steps \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# 타임스텝 수\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 결과 시각화\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 저장 디렉토리 확인\n",
    "os.makedirs('./ckpt/diffusion_y', exist_ok=True)\n",
    "\n",
    "# 차원 정보 출력\n",
    "print(f\"Action shape: {action.shape}\")\n",
    "print(f\"GT action shape: {gt_action.shape}\")\n",
    "\n",
    "action_dim = action.shape[2]  # 액션 차원 수\n",
    "time_steps = action.shape[1]  # 타임스텝 수\n",
    "\n",
    "# 결과 시각화\n",
    "fig, axs = plt.subplots(action_dim, 1, figsize=(15, 15))\n",
    "\n",
    "for i in range(action_dim):\n",
    "    # 여러 개의 서브플롯이 있을 때와 하나만 있을 때를 구분\n",
    "    if action_dim == 1:\n",
    "        ax = axs\n",
    "    else:\n",
    "        ax = axs[i]\n",
    "    \n",
    "    # 예측 액션: 배치[0], 모든 타임스텝[:], 액션차원[i] 선택\n",
    "    pred_action = action[0, :, i].cpu().detach().numpy()\n",
    "    \n",
    "    # 실제 액션: gt_action은 단일 타임스텝 형태이므로 그냥 액션 차원만 선택\n",
    "    # 같은 값을 모든 타임스텝에 확장\n",
    "    real_action = np.full(time_steps, gt_action[0, i].cpu().detach().numpy())\n",
    "    \n",
    "    ax.plot(pred_action, label=\"prediction\")\n",
    "    ax.plot(real_action, label=\"ground truth\")\n",
    "    ax.set_title(f\"Action Dimension {i}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./ckpt/diffusion_y/action_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# 오류 히스토그램 (선택 사항)\n",
    "error = np.abs(pred_action - real_action)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(error, bins=30)\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Error Distribution')\n",
    "plt.savefig('./ckpt/diffusion_y/error_histogram.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
