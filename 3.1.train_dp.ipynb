{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Diffusion Policy on your Dataset\n",
    "Train the Diffusion Policy model on your custom dataset. In this example, we set horizon size as 10."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.common.datasets.factory import resolve_delta_timestamps\n",
    "import torchvision"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Number of offline training steps (we'll only do offline training for this example.)\n",
    "# Adjust as you prefer. 5000 steps are needed to get something worth evaluating.\n",
    "training_steps = 3000\n",
    "log_freq = 100"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Configuration and Initialize\n",
    "\n",
    "horizon = 10"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# When starting from scratch (i.e. not from a pretrained policy), we need to specify 2 things before\n",
    "# creating the policy:\n",
    "#   - input/output shapes: to properly size the policy\n",
    "#   - dataset stats: for normalization and denormalization of input/outputs\n",
    "dataset_metadata = LeRobotDatasetMetadata(\"omy_pnp\", root='./demo_data')\n",
    "features = dataset_to_policy_features(dataset_metadata.features)\n",
    "output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "input_features = {key: ft for key, ft in features.items() if key not in output_features}\n",
    "input_features.pop(\"observation.wrist_image\")\n",
    "\n",
    "# Policies are initialized with a configuration class, in this case . For this example,\n",
    "# we'll just use the defaults and so no arguments other than input/output features need to be passed.\n",
    "cfg = DiffusionConfig(input_features=input_features, output_features=output_features, horizon=10, n_action_steps=10)\n",
    "\n",
    "# This allows us to construct the data with action chunking\n",
    "delta_timestamps = resolve_delta_timestamps(cfg, dataset_metadata)\n",
    "\n",
    "# We can now instantiate our policy with this config and the dataset stats.\n",
    "policy = DiffusionPolicy(cfg, dataset_stats=dataset_metadata.stats)\n",
    "policy.train()\n",
    "policy.to(device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., std=0.01):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # Adds noise: tensor remains a tensor.\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        return tensor + noise\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "# Create a transformation pipeline that converts a PIL image to a tensor, then adds noise.\n",
    "transform = transforms.Compose([\n",
    "    AddGaussianNoise(mean=0., std=0.02),\n",
    "    transforms.Lambda(lambda x: x.clamp(0, 1))\n",
    "])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We can then instantiate the dataset with these delta_timestamps configuration.\n",
    "dataset = LeRobotDataset(\"omy_pnp\", delta_timestamps=delta_timestamps, root='./demo_data', image_transforms=transform)\n",
    "\n",
    "# Then we create our optimizer and dataloader for offline training.\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    drop_last=True,\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The trained checkpoint will be saved in './ckpt/diffusion_y' folder."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run training loop.\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    for batch in dataloader:\n",
    "        inp_batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        loss, _ = policy.forward(inp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % log_freq == 0:\n",
    "            print(f\"step: {step} loss: {loss.item():.3f}\")\n",
    "        step += 1\n",
    "        if step >= training_steps:\n",
    "            done = True\n",
    "            break\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the policy to disk.\n",
    "policy.save_pretrained('./ckpt/diffusion_y')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference\n",
    "\n",
    "To evaluate the policy on the dataset, you can calculate the error between ground-truth actions from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "class EpisodeSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset: LeRobotDataset, episode_index: int):\n",
    "        from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "        to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "        self.frame_ids = range(from_idx, to_idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.frame_ids)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frame_ids)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "policy.eval()\n",
    "actions = []\n",
    "gt_actions = []\n",
    "images = []\n",
    "episode_index = 0\n",
    "episode_sampler = EpisodeSampler(dataset, episode_index)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    sampler=episode_sampler,\n",
    ")\n",
    "policy.reset()\n",
    "for batch in test_dataloader:\n",
    "    inp_batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "    action = policy.select_action(inp_batch)\n",
    "    actions.append(action)\n",
    "    gt_actions.append(inp_batch[\"action\"][:,0,:])\n",
    "    images.append(inp_batch[\"observation.image\"])\n",
    "actions = torch.cat(actions, dim=0)\n",
    "gt_actions = torch.cat(gt_actions, dim=0)\n",
    "print(f\"Mean action error: {torch.mean(torch.abs(actions - gt_actions)).item():.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}